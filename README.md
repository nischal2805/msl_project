# ISL Real-Time Translation

**Real-time Indian Sign Language to English text translation model optimized for mobile deployment.**

![Architecture](docs/architecture.png)

## ğŸ“‹ Overview

This project implements a complete pipeline for translating Indian Sign Language (ISL) videos to English text. The model is specifically designed for:
- **Real-time inference** on mobile devices (Samsung Galaxy Tab S9)
- **Small footprint** (~16M parameters, ~20MB INT8 quantized)
- **High accuracy** using MobileNetV3 encoder + Transformer decoder

## ğŸ—ï¸ Architecture

```
Video Frames (224Ã—224Ã—3, 16 frames)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MobileNetV3-Large         â”‚ â† Frame feature extraction (5.4M params)
â”‚   (pretrained on ImageNet)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Temporal Convolutions     â”‚ â† Motion modeling (critical for sign language)
â”‚   (depthwise separable)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Attention Pooling         â”‚ â† Compress to 32 tokens (2.1M params)
â”‚   (learnable queries)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transformer Decoder       â”‚ â† Text generation (8.2M params)
â”‚   (4 layers, Pre-LN)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
      Text Output
```

## ğŸš€ Quick Start

### 1. Installation

```powershell
# Navigate to project directory
cd isl_realtime

# Create virtual environment (optional)
python -m venv venv
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Paths

Edit `configs/config.yaml`:

```yaml
data:
  video_dir: "D:/datasets/iSign-videos"     # Your video directory
  csv_path: "D:/datasets/iSign_v1.1.csv"    # Your CSV path
```

### 3. Train

```powershell
# Train from scratch
python scripts/train.py --config configs/config.yaml

# Resume training
python scripts/train.py --config configs/config.yaml --resume checkpoints/best_model.pt
```

### 4. Evaluate

```powershell
python scripts/evaluate.py --checkpoint checkpoints/best_model.pt
```

### 5. Demo

```powershell
# Interactive mode
python scripts/demo.py --checkpoint checkpoints/best_model.pt --mode interactive

# Live camera
python scripts/demo.py --checkpoint checkpoints/best_model.pt --mode live

# Single video
python scripts/demo.py --checkpoint checkpoints/best_model.pt --mode video --input path/to/video.mp4
```

### 6. Export for Mobile

```powershell
python scripts/export.py --checkpoint checkpoints/best_model.pt --format onnx --quantize
```

## ğŸ“ Project Structure

```
isl_realtime/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml           # All hyperparameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py        # MobileNetV3 + Temporal Attention
â”‚   â”‚   â”œâ”€â”€ decoder.py        # Transformer Decoder
â”‚   â”‚   â””â”€â”€ translator.py     # Full model
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py        # Video dataset
â”‚   â”‚   â””â”€â”€ augmentations.py  # Video augmentations
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py        # Training loop
â”‚   â”‚   â””â”€â”€ losses.py         # CTC + CE loss
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ live.py           # Real-time inference
â”‚       â””â”€â”€ export.py         # ONNX/TFLite export
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation script
â”‚   â”œâ”€â”€ demo.py               # Demo script
â”‚   â”œâ”€â”€ export.py             # Export script
â”‚   â””â”€â”€ tune.py               # Hyperparameter tuning
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Š Expected Results

| Epoch | Val Loss | Expected BLEU |
|-------|----------|---------------|
| 10    | ~3.0     | 5-10          |
| 30    | ~2.0     | 15-25         |
| 50    | ~1.5     | 25-35         |

**Note:** Sign Language Translation is challenging. BLEU scores of 25-35 are considered good for this task.

## ğŸ”§ Key Features

### Training
- âœ… Mixed precision training (AMP)
- âœ… Gradient accumulation
- âœ… Backbone freezing/unfreezing
- âœ… Early stopping
- âœ… Automatic checkpointing
- âœ… Separate learning rates for encoder/decoder

### Model
- âœ… MobileNetV3 backbone (mobile-optimized)
- âœ… Temporal convolutions (motion modeling)
- âœ… Attention pooling (variable to fixed length)
- âœ… Pre-LayerNorm Transformer (stable training)
- âœ… CTC + CE hybrid loss

### Inference
- âœ… Greedy decoding
- âœ… Beam search
- âœ… Temperature sampling
- âœ… CTC streaming mode

### Export
- âœ… ONNX export
- âœ… INT8 quantization
- âœ… TorchScript
- ğŸ”„ TFLite (via ONNX)

## ğŸ’¡ Key Improvements from Previous Attempts

| Issue | Previous | Fixed |
|-------|----------|-------|
| Encoder output length | 1568 tokens | 32 tokens (attention pooling) |
| BOS/EOS mismatch | Inconsistent IDs | Always use 101/102 (BERT) |
| Metrics | Teacher-forcing BLEU | Autoregressive BLEU |
| Model size | ~110M params | ~16M params |
| Temporal modeling | None | Temporal convolutions |
| Training stability | Unstable | Pre-LN Transformer |

## ğŸ“± Mobile Deployment

After training, export the model:

```powershell
# Export with INT8 quantization
python scripts/export.py --checkpoint checkpoints/best_model.pt --format onnx --quantize
```

The exported model can be used with:
- **Android**: ONNX Runtime for Android
- **iOS**: Core ML (convert from ONNX)

Expected mobile performance:
- **Model size**: ~20MB (INT8)
- **Inference time**: ~100-200ms per video segment

## ğŸ”¬ Hyperparameter Tuning

Run random search to find optimal hyperparameters:

```powershell
python scripts/tune.py --config configs/config.yaml --trials 20 --epochs 5
```

## âš ï¸ Common Issues

### CUDA Out of Memory
Reduce batch size in `config.yaml`:
```yaml
training:
  batch_size: 16  # Reduce from 32
```

### Slow Training
- Enable mixed precision: `use_amp: true`
- Reduce `num_workers` if I/O bound
- Use SSD for video storage

### Poor Results
1. Check data paths in config
2. Ensure videos have consistent quality
3. Increase training epochs
4. Try hyperparameter tuning

## ğŸ“„ License

This project is for educational purposes (5th semester project).

## ğŸ™ Acknowledgments

- iSign dataset from HuggingFace
- MobileNetV3 pretrained weights from torchvision
- BERT tokenizer from HuggingFace Transformers
